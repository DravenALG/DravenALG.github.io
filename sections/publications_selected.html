<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;"><tbody>
  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%; vertical-align:middle">
      <img src="images/PanoDecouple.jpg" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://arxiv.org/abs/2503.18420" target="_blank">
        <span class="papertitle">Panorama Generation From NFoV Image Done Right.</span>
      </a>
      <br>
      Dian Zheng, Cheng Zhang, <strong>Xiao-Ming Wu</strong>, Cao Li, Chengfei Lv, Jian-Fang Hu, Wei-Shi Zheng*.
      <br>
      <em>CVPR</em>, 2025, <strong>(Hightlight)</strong>.
      <br>
      <a href="https://isee-laboratory.github.io/PanoDecouple" target="_blank">page</a>
      /
      <a href="https://arxiv.org/abs/2503.18420" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/PanoDecouple" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Panorama Generation)</strong> Make your panorama generation done rightüåÑ.
      </p>
    </td>
  </tr>
  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/DiffuVolume.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://arxiv.org/abs/2308.15989" target="_blank">
        <span class="papertitle">DiffuVolume: Diffusion Model for Volume based Stereo Matching.</span>
      </a>
      <br>
      Dian Zheng, <strong>Xiao-Ming Wu</strong>, Zuhao Liu, Jingke Meng, Wei-Shi Zheng*.
      <br>
      <em>IJCV</em>, 2025.
      <br>
      <a href="https://arxiv.org/abs/2308.15989" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/DiffuVolume" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Stereo Matching)</strong> Boost your stereo matching methods with our lightweight, plug-and-play DiffuVolumeüòÄ.
      </p>
    </td>
  </tr>
  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/MotionGrasp.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">
        <span class="papertitle">MotionGrasp: Long-Term Grasp Motion Tracking for Dynamic Grasping.</span>
      </a>
      <br>
      Nuo Chen&, <strong>Xiao-Ming Wu&</strong>, Guo-Hao Xu, Jian-Jian Jiang, Zibo Chen, Wei-Shi Zheng*.
      <br>
      <em>RA-L</em>, 2024.
      <br>
      <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">paper</a>
      /
      <a href="https://github.com/ChenN-Scott/MotionGrasp" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Dynamic Grasping)</strong> Track and grasp the moving objects with grasp motionüêá.
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
      <img src="images/DexGYS.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://arxiv.org/abs/2405.19291" target="_blank">
        <span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation.</span>
      </a>
      <br>
      Yi-Lin Wei, Jian-Jian Jiang, Chengyi Xing, Xiantuo Tan, <strong>Xiao-Ming Wu</strong>, Hao Li, Mark Cutkosky, Wei-Shi Zheng*.
      <br>
      <em>NeurIPS</em>, 2024.
      <br>
      <a href="https://isee-laboratory.github.io/DexGYS/" target="_blank">page</a>
      /
      <a href="https://arxiv.org/abs/2405.19291" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/Grasp-as-You-Say" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Dexterous Grasping)</strong> Guide dexterous grasping with what you sayüéôÔ∏è.
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/EconomicGrasp.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-73383-3_21" target="_blank">
        <span class="papertitle">An Economic Framework for 6-DoF Grasp Detection.</span>
      </a>
      <br>
      <strong>Xiao-Ming Wu&</strong>, Jia-Feng Cai&, Jian-Jian Jiang, Dian Zheng, Yi-Lin Wei, Wei-Shi Zheng*.
      <br>
      <em>ECCV</em>, 2024.
      <br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-73383-3_21" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(6-DoF Grasping)</strong> Speed up your 6-DoF grasping training 10x without performance dropüöÑ.
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/DiffUIR.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">
        <span class="papertitle">Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model.</span>
      </a>
      <br>
      Dian Zheng, <strong>Xiao-Ming Wu</strong>, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng*.
      <br>
      <em>CVPR</em>, 2024.
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Image Restoration)</strong> Hourglass diffusion is a good way for universal image restorationüåÜ.
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/DGTR.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">
        <span class="papertitle">Dexterous Grasp Transformer.</span>
      </a>
      <br>
      Guo-Hao Xu&, Yi-Lin Wei&, Dian Zheng, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*.
      <br>
      <em>CVPR</em>, 2024.
      <br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">paper</a>
      /
      <a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Dexterous Grasping)</strong> Generate a diverse set of feasible dexterous grasp only in one passü¶æ!
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/ReSTE.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">
        <span class="papertitle">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training.</span>
      </a>
      <br>
      <strong>Xiao-Ming Wu</strong>, Dian Zheng, Zuhao Liu, Wei-Shi Zheng*.
      <br>
      <em>ICCV</em>, 2023.
      <br>
      <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">paper</a>
      /
      <a href="https://github.com/DravenALG/ReSTE" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Binary Neural Networks)</strong> Simple and effective gradient estimator for binary neural network trainingü§©!
      </p>
    </td>
  </tr>

  <tr>
    <td class="paper_image" style="padding-bottom: 20px; width:25%;vertical-align:middle">
      <img src="images/OASIS.png" alt="PontTuset" width="200" style="border-style: none">
    </td>
    <td style="width:75%; vertical-align:middle">
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">
        <span class="papertitle">Online Enhanced Semantic Hashing Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data.</span>
      </a>
      <br>
      <strong>Xiao-Ming Wu</strong>, Xin Luo*, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu.
      <br>
      <em>AAAI</em>, 2022.
      <br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">paper</a>
      /
      <a href="https://github.com/DravenALG/OASIS" target="_blank">code</a>
      <p style="margin-bottom: 15px; margin-top: 5px">
        <strong>(Hashing Retrieval)</strong> New benchmark and baseline for online multi-modal hashingüôå.
      </p>
    </td>
  </tr>
</tbody></table>
