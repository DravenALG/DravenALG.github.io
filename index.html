<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiao-Ming Wu</title>

    <meta name="author" content="Xiao-Ming Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/Naruto.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel='stylesheet' href='https://chinese-fonts-cdn.deno.dev/packages/xuandongkaishu/dist/XuandongKaishu/result.css' />
    
  </head>

  <body>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: 30px"><tbody>
      <tr style="padding:0px">
        <td class="fixed-header" style="font-size: larger">
          <a href="https://dravenalg.github.io" style="padding-right: 150px; font-size: large;" class="header_name"><img src="images/Naruto.png" style="width: 20px"> Xiao-Ming Wu</a>
          <a href="#bio" style="padding-right: 30px;padding-left: 30px" class="header_navi">Bio</a>
          <a href="#publication" style="padding-right: 30px;padding-left: 30px" class="header_navi">Publications</a>
          <a href="#service" style="padding-right: 30px;padding-left: 30px" class="header_navi">Services</a>
          <a href="#award" style="padding-right: 30px;padding-left: 30px" class="header_navi">Awards</a>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td class="personal_image" style="padding:2.5%; width:40%;max-width:40%">
                <img style="width:70%;max-width:100%;object-fit: cover;border-radius: 200px" alt="profile photo" src="images/myself.jpg" class="personal_img">
              </td>

              <td style="width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Xiao-Ming Wu <span style="padding-left: 5px" class="chinese_name">伍晓鸣</span>
                </p>
                <p>
                  M.S. student
                </p>
                <p>
                  School of Computer Science and Engineering
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  Email: wuxm65@mail2.sysu.edu.cn
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ" target="_blank">
                    [<span style="color: #4285F4; margin-right: -2px">G</span>
                    <span style="color: #EA4335; margin-right: -2px">o</span>
                    <span style="color: #FBBC05; margin-right: -2px">o</span>
                    <span style="color: #4285F4; margin-right: -2px">g</span>
                    <span style="color: #34A853; margin-right: -2px">l</span>
                    <span style="color: #EA4335; margin-right: -2px">e</span> &nbsp Scholar]</a> &nbsp;&nbsp;
                  <a href="https://github.com/DravenALG" target="_blank"><strong style="color: black">[GitHub]</strong></a>
                </p>
              </td>

            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2 id="bio" style="scroll-margin-top: 45px;">
                  Biography
                </h2>
                <p>
                  I'm currently a third-year master student at <a href="https://www.sysu.edu.cn" target="_blank">Sun Yat-sen University</a>, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>, where I develop the scientific ability and a good taste of it.
                  I was fortunate to visit <a href="https://www.mq.edu.au" target="_blank">Macquarie university</a>, enjoying a happy and fruitful research journey with Prof. <a href="https://researchers.mq.edu.au/en/persons/longbing-cao" target="_blank">Longbing Cao</a>.
                  Previously, I obtain my B.E. degree in <a href="https://www.sdu.edu.cn/index.htm" target="_blank">Shandong University</a>. At that time, I had my first attempt on scientific research and cultivate the interest in it, advised by Prof. <a href="http://mima.sdu.edu.cn/Members/xinshunxu/index.htm" target="_blank">Xin-Shun Xu</a> and Associate Prof. <a href="https://faculty.sdu.edu.cn/luoxin/zh_CN/index.htm" target="_blank">Xin Luo</a>.
                </p>

                <p>
                  <em class="ref">
                    Research is for curiosity and fun.
                  </em>
                </p>

                <p>
                  I am interested in general deep learning topics, including algorithms, theories, systems and its applications on vision, language scenarios.
                  My long-term goal is to build a strong, efficient (reach human-level), and safe (surpass human level) AI agent.
                  Now I am mainly focusing on embodied AI. Previously, I also conducted research on model compression and hashing retrieval.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                News
              </h2>
              <div class="scrollable" style="max-height:130px; overflow-y:scroll; padding-right:10px; margin-top: 10px; margin-bottom: 10px">
                <p>
                  <span style="font-size: smaller">➤</span> [2024-12] One paper accepted in AAAI 2025.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-11] One paper accepted in RA-L 2024 (My co-first author work MotionGrasp).
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-09] One paper accepted in NeurIPS 2024.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-09] One paper accepted in CoRL 2024.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-07] One paper accepted in ACM Multimedia 2024.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-07] One paper accepted in ECCV 2024 (my first-author work EconomicGrasp).
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2024-02] Three papers accepted in CVPR 2024.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2023-07] One paper accepted in ICCV 2023 (my first-author work ReSTE).
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2023-02] One paper accepted in CVPR 2023.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2022-02] One paper accepted in Pattern Recognition.
                </p>
                <p>
                  <span style="font-size: smaller">➤</span> [2021-12] One paper accepted in AAAI 2022 (my first-author work OASIS).
                </p>
              </div>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 0px; margin-bottom: -10px;"><tbody>
            <tr>
              <td style="width:80%; vertical-align:middle">
                <h2 id="publication" style="scroll-margin-top: 45px;">Selected Publications</h2>
                <p>
                  Below are my selected publications.
                  (& means equal contribution, * refers to corresponding author.)
                </p>
              </td>
            </tr>
           </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/MotionGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">
                  <span class="papertitle">MotionGrasp: Long-Term Grasp Motion Tracking for Dynamic Grasping.</span>
                </a>
                <br>
                Nuo Chen&, <strong>Xiao-Ming Wu&</strong>, Guo-Hao Xu, Jian-Jian Jiang, Zibo Chen, Wei-Shi Zheng*.
                <br>
                <em>Robotics and Automation Letters (RA-L)</em>, 2024.
                <br>
                <a href="https://ieeexplore.ieee.org/document/10764717" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dynamic Grasp)</strong> We design a new grasp tracking framework for dynamic grasping, fully considering long-term trajectory information and using Grasp Motion to model it well, which consists of a motion association and a motion alignment module to enable it.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DexGYS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">
                  <span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation</span>
                </a>
                <br>
                Yi-Lin Wei, Jian-Jian Jiang, Chengyi Xing, Xiantuo Tan, <strong>Xiao-Ming Wu</strong>, Hao Li, Mark Cutkosky, Wei-Shi Zheng*.
                <br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2024.
                <br>
                <a href="https://isee-laboratory.github.io/DexGYS/" target="_blank">page</a>
                /
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/Grasp-as-You-Say" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dexterous Grasp)</strong> We propose a novel task "Dexterous Grasp as You Say" (DexGYS), with a benchmark and a framework, enabling robots to perform dexterous grasping based on human commands expressed in natural language.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/R2SGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openreview.net/forum?id=uJBMZ6S02T" target="_blank">
                  <span class="papertitle">Real-to-Sim Grasp: Rethinking the Gap between Simulation and Real World in Grasp Detection</span>
                </a>
                <br>
                Jia-Feng Cai, Zibo Chen, <strong>Xiao-Ming Wu</strong>, Jian-Jian Jiang, Yi-Lin Wei, Wei-Shi Zheng*.
                <br>
                <em>Conference on Robot Learning (CoRL)</em>, 2024.
                <br>
                <a href="https://isee-laboratory.github.io/R2SGrasp" target="_blank">page</a>
                /
                <a href="https://openreview.net/forum?id=uJBMZ6S02T" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(6-DoF Grasp)</strong> We propose a Real-to-Sim framework for 6-DoF Grasp detection, named R2SGrasp, with the key insight of bridging this gap in a real-to-sim way, and build a large-scale simulated dataset to pretrain our model to achieve great real-world performance.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/PixelFade.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681262" target="_blank">
                  <span class="papertitle">PixelFade: Privacy-preserving Person Re-identification with Noise-guided Progressive Replacement</span>
                </a>
                <br>
                Delong Zhang, Yi-Xing Peng, <strong>Xiao-Ming Wu</strong>, Ancong Wu*, Wei-Shi Zheng.
                <br>
                <em>ACM Multimedia (MM)</em>, 2024.
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681262" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/PixelFade" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Privacy Protection)</strong> We propose a two-step iterative method (PixelFade) for person privacy-preserving, with the partial replacement step to turn image into noise to resist recovery attack, and the constrain operation step to maintain Re-id semantics.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/EconomicGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-73383-3_21" target="_blank">
                  <span class="papertitle">An Economic Framework for 6-DoF Grasp Detection</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu&</strong>, Jia-Feng Cai&, Jian-Jian Jiang, Dian Zheng, Yi-Lin Wei, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-73383-3_21" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(6-DoF Grasp)</strong> We propose a new economic grasping framework for 6-DoF grasp detection to economize the training resource cost and meanwhile maintain effective grasp performance.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DiffUIR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Image Restoration)</strong> We propose a diffusion-based universal image restoration model, with an assemble-then-separate (like the hourglass) mapping for multi-task training, to learn the shared information between different tasks.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DGTR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Dexterous Grasp Transformer</span>
                </a>
                <br>
                Guo-Hao Xu&, Yi-Lin Wei&, Dian Zheng, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dexterous Grasp)</strong> We propose a new transformer-based framework for dexterous grasp generation, capable of predicting a diverse set of feasible grasp poses only in one pass.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/ReSTE.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">
                  <span class="papertitle">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Dian Zheng, Zuhao Liu, Wei-Shi Zheng*
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/ReSTE" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Binary Neural Networks)</strong> We propose a new perspective to view the binary neural network training: equilibrium between estimating error and gradient stability, and design a simple and effective gradient estimator (ReSTE) to balance it well.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DOCH.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">
                  <span class="papertitle">Discrete Online Cross-modal Hashing</span>
                </a>
                <br>
                Yu-Wei Zhan, Yongxin Wang, Yu Sun, <strong>Xiao-Ming Wu</strong>, Xin Luo*, Xin-Shun Xu
                <br>
                <em>Pattern Recognition (PR)</em>, 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">paper</a>
                /
                <a href="https://github.com/yw-zhan/DOCH" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Retrieval)</strong> We propose a novel cross-modal online hashing method, directly exploiting the similarity between newly coming data and old existing data in the Hamming space to learn discriminate hashing codes.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/OASIS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">
                  <span class="papertitle">Online Enhanced Semantic Hashing Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Xin Luo*, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/OASIS" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Retrieval)</strong> We explore a new task, incremental multi-modal hashing, and introduce semantics to solve this task, handling the dimension mismatching problem and mitigating the inconsistent problem that occurs when new classes come.
                </p>
              </td>
            </tr>


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="service" style="scroll-margin-top: 45px;">
                Services and Activities
              </h2>
              <p>
                <div style="padding-bottom: 5px"><strong>Journal Reviewer:</strong></div>
                <div style="padding-bottom: 5px">Pattern Analysis and Machine Intelligence (TPAMI)</div>
                <div style="padding-bottom: 5px">Pattern Recognition (PR)</div>
              </p>
              <p>
                <div style="padding-bottom: 5px"><strong>Conference Reviewer:</strong></div>
                <div style="padding-bottom: 5px">Computer Vision and Pattern Recognition (CVPR) 2024, 2025</div>
                <div style="padding-bottom: 5px">ACM Multimedia (MM) 2024</div>
                <div style="padding-bottom: 5px">International Conference on Multimedia&Expo (ICME) 2025</div>
              </p>
            </td>
          </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="award" style="scroll-margin-top: 45px;">
                Selected Awards
              </h2>
              <p>

              <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Sun Yat-Sen University for Graduate Student (中山大学硕士研究生一等奖助金), 2022, 2023, 2024. </div>
              <div style="padding-bottom: 5px"> National Scholarship of China for Graduate Student (研究生国家奖学金), 2023. </div>
                <div style="padding-bottom: 5px"> Honorable Bachelor Degree of Shandong University (山东大学荣誉学士学位), 2022. </div>
                <div style="padding-bottom: 5px"> National Scholarship of China for Undergraduate Student  (本科生国家奖学金), 2019, 2021. </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Shandong University for Undergraduate Student (山东大学本科生学业一等奖学金), 2019, 2020, 2021. </div>
              </p>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>

    <script src="script.js"></script>

  </body>
</html>
