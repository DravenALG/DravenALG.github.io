<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <title>Xiao-Ming Wu</title>

    <meta name="author" content="Xiao-Ming Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/Naruto.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: 20px"><tbody>
      <tr style="padding:0px">
        <td>
          <div class="fixed-header">
            <a href="https://dravenalg.github.io" style="padding-right: 150px;" class="header_name">Xiao-Ming Wu</a>
            <a href="#bio" style="padding-right: 30px;padding-left: 30px" class="header_navi">Bio</a>
            <a href="#publication" style="padding-right: 30px;padding-left: 30px" class="header_navi">Publications</a>
            <a href="#service" style="padding-right: 30px;padding-left: 30px" class="header_navi">Services</a>
            <a href="#award" style="padding-right: 30px;padding-left: 30px" class="header_navi">Awards</a>
          </div>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td class="personal_image" style="padding:2.5%; width:40%;max-width:40%">
                <a href="images/myself.jpg" target="_blank"><img style="width:75%;max-width:100%;object-fit: cover;border-radius: 200px" alt="profile photo" src="images/myself.jpg" class="hoverZoomLink"></a>
              </td>

              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Xiao-Ming Wu (伍晓鸣)
                </p>
                <p>
                  M.S. student
                </p>
                <p>
                  School of Computer Science and Engineering
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  Email: wuxm65@mail2.sysu.edu.cn
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ" target="_blank">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/DravenALG" target="_blank">Github</a>
                </p>
              </td>

            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2 id="bio">
                  Biography
                </h2>
                <p>
                  I'm currently a second-year master student at <a href="https://www.sysu.edu.cn" target="_blank">Sun Yat-sen University</a>, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>, where I develop the scientific ability and a good taste of it.
                  Now I am also a visiting student in <a href="https://www.mq.edu.au" target="_blank">Macquarie university</a>, enjoying an interesting research journey with Prof. <a href="https://researchers.mq.edu.au/en/persons/longbing-cao" target="_blank">Longbing Cao</a>.
                  Previously, I obtain my B.E. degree in <a href="https://www.sdu.edu.cn/index.htm" target="_blank">Shandong University</a>. At that time, I had my first attempt on scientific research and cultivate the interest in it, advised by Prof. <a href="http://mima.sdu.edu.cn/Members/xinshunxu/index.htm" target="_blank">Xin-Shun Xu</a> and Prof. <a href="https://faculty.sdu.edu.cn/luoxin/zh_CN/index.htm" target="_blank">Xin Luo</a>.
                </p>

                <p>
                  <em class="ref">
                    Research is for curiosity and fun.
                  </em>
                </p>

                <p>
                  I am interested in general deep learning topics, including algorithms, theories, systems and its applications on vision, language scenarios.
                  My long-term goal is to build a strong, efficient (reach human-level), and safe (surpass human level) AI agent.
                  Now I am mainly focusing on efficient embodied AI systems. Previously, I also conducted research on information retrieval.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                News
              </h2>
              <div class="scrollable" style="max-height:130px; overflow-y:scroll; padding-right:10px; margin-top: 10px; margin-bottom: 10px">
                <p>
                  One paper accepted in CoRL 2024.
                </p>
                <p>
                  One paper accepted in ICPR 2024.
                </p>
                <p>
                  One paper accepted in ACM Multimedia 2024.
                </p>
                <p>
                  One paper accepted in ECCV 2024 (my first-author work).
                </p>
                <p>
                  Three papers accepted in CVPR 2024.
                </p>
                <p>
                  One paper accepted in ICCV 2023 (my first-author work).
                </p>
                <p>
                  One paper accepted in CVPR 2023.
                </p>
                <p>
                  One paper accepted in CIKM 2022.
                </p>
                <p>
                  One paper accepted in AAAI 2022 (my first-author work).
                </p>
                <p>
                  One paper accepted in Pattern Recognition.
                </p>
              </div>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 0px; margin-bottom: -10px;"><tbody>
            <tr>
              <td style="width:80%; vertical-align:middle">
                <h2 id="publication">Selected Publications</h2>
                <p>
                  Below are my selected publications.
                  (& means equal contribution, * refers to corresponding author.)
                </p>
              </td>
            </tr>
           </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/PixelFade.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2408.05543" target="_blank">
                  <span class="papertitle">PixelFade: Privacy-preserving Person Re-identification with Noise-guided Progressive Replacement</span>
                </a>
                <br>
                Delong Zhang, Yi-Xing Peng, <strong>Xiao-Ming Wu</strong>, Ancong Wu*, Wei-Shi Zheng.
                <br>
                <em>ACM Multimedia (MM)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2408.05543" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/PixelFade" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Privacy Protection)</strong> We propose a two-step iterative method (PixelFade) for person privacy-preserving, with the partial replacement step to turn image into noise to resist recovery attack, and the constrain operation step to maintain Re-id semantics.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/EconomicGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2407.08366" target="_blank">
                  <span class="papertitle">An Economic Framework for 6-DoF Grasp Detection</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu&</strong>, Jia-Feng Cai&, Jian-Jian Jiang, Dian Zheng, Yi-Lin Wei, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2407.08366" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(6-DoF Grasp)</strong> We propose a new economic grasping framework for 6-DoF grasp detection to economize the training resource cost and meanwhile maintain effective grasp performance, which consists of a novel label selection strategy and a focal module to enable it.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DiffUIR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Image Restoration)</strong> We propose a diffusion-based universal image restoration model, with an assemble-then-separate (like the hourglass) mapping for multi-task training, to learn the shared information between different tasks.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DGTR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Dexterous Grasp Transformer</span>
                </a>
                <br>
                Guo-Hao Xu<sup>&</sup>, Yi-Lin Wei<sup>&</sup>, Dian Zheng, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dexterous Grasp)</strong> We propose a new transformer-based framework for dexterous grasp generation, capable of predicting a diverse set of feasible grasp poses only in one pass.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/S2HGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Single-View Scene Point Cloud Human Grasp Generation</span>
                </a>
                <br>
                Yan-Kang Wang, Chengyi Xing, Yi-Lin Wei, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/S2HGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Human Grasp)</strong> We explore a new task of generating human grasps based on single-view scene point clouds. And we design a new diffusion-based baseline and a new dataset for this novel task.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/ReSTE.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">
                  <span class="papertitle">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Dian Zheng, Zuhao Liu, Wei-Shi Zheng*
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/ReSTE" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Binary Neural Networks)</strong> We propose a new perspective to view the binary neural network training: equilibrium between estimating error and gradient stability, and design a simple and effective gradient estimator (ReSTE) to balance it well.
                </p>
              </td>
            </tr>

            <tr>
              <td class="paper_image" style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/OASIS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">
                  <span class="papertitle">Online Enhanced Semantic Hashing Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Xin Luo*, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/OASIS" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Retrieval)</strong> We explore a new task, incremental multi-modal hashing, and introduce semantics to solve this task, handling the dimension mismatching problem and mitigating the inconsistent problem that occurs when new classes come.
                </p>
              </td>
            </tr>


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="service">
                Services and Activities
              </h2>
              <p>
                Journal Reviewer: Pattern Analysis and Machine Intelligence (TPAMI), Pattern Recognition (PR).
              </p>
              <p>
                Conference Reviewer: Computer Vision and Pattern Recognition (CVPR) 2024, ACM Multimedia (MM) 2024.
              </p>
            </td>
          </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2 id="award">
                Selected Awards
              </h2>
              <p>
                <div style="padding-bottom: 5px"> National Scholarship of China for Graduate Student (研究生国家奖学金), 2023 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Sun Yat-Sen University (中山大学学业一等奖学金), 2022, 2023 </div>
                <div style="padding-bottom: 5px"> Honorable Bachelor Degree of Shandong University (山东大学荣誉学士学位), 2022 </div>
                <div style="padding-bottom: 5px"> National Scholarship of China for Undergraduate Student  (本科生国家奖学金), 2019, 2021 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Shandong University (山东大学学业一等奖学金), 2019, 2020, 2021 </div>
              </p>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>

  </body>
</html>
