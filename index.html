<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiao-Ming Wu</title>

    <meta name="author" content="Xiao-Ming Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/leaf.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/myself.jpg" target="_blank"><img style="width:85%;max-width:100%;object-fit: cover;border-radius: 75px" alt="profile photo" src="images/myself.jpg" class="hoverZoomLink"></a>
              </td>

              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Xiao-Ming Wu (伍晓鸣)
                </p>
                <p>
                  M.S. student
                </p>
                <p>
                  School of Computer Science and Engineering
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  wuxm65@mail2.sysu.edu.cn
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ" target="_blank">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/DravenALG" target="_blank">Github</a>
                </p>
              </td>

            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>
                  Biography
                </h2>
                <p>
                  I'm currently a second-year master student at <a href="https://www.sysu.edu.cn" target="_blank">Sun Yat-sen University</a>, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>, where I cultivate the interest in research, and develop the scientific ability and taste of it.
                  Previously, I obtain my B.E. degree in <a href="https://www.sdu.edu.cn/index.htm" target="_blank">Shandong University</a>. At that time, I had my first attempt on scientific research, advised by <a href="https://faculty.sdu.edu.cn/luoxin/zh_CN/index.htm" target="_blank">Xin Luo</a> and <a href="http://mima.sdu.edu.cn/Members/xinshunxu/index.htm" target="_blank">Xin-Shun Xu</a>.
                  <strong>I am now looking for a potential Ph.D. position in 2025 Fall.</strong>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  <em class="ref">
                    Research is for curiosity and fun.
                  </em>
                </p>
                <p>
                  My AI dream is to build a strong, efficient (to reach human-level intelligence), and safe (beyond the human) AI agent. And below is the roadmap:
                </p>
                <ul>
                  <li style="padding-bottom: 4px"><strong>Embodied AI:</strong> let the AI agent have the embodied ability to interact with and feel the real world.</li>
                  <li><strong>Representation Learning:</strong> and on the basic on embodied ability, design new supervision, architecture or optimizer, to enable the strong, efficient and save AI agent. </li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:25%; vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  Below are my publications. My first author works are <span class="highlight">highlighted</span>.
                  <br>Including papers in arXiv, & means equal contribution, * refers to corresponding author.
                </p>
              </td>
            </tr>

            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Embodied AI</h3>
              </td>
            </tr>


            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="" target="_blank">
                  <span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation</span>
                </a>
                <br>
                Yi-Lin Wei, Jian-Jian Jiang, Chengyi Xing, Xiantuo Tan, <strong>Xiao-Ming Wu</strong>, Hao Li, Mark Cutkosky, Wei-Shi Zheng*
                <br>
                <em>arXiv preprint arXiv:2405.19291</em>
                <br>
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">paper</a>
                <p style="margin-bottom: 0px; margin-top: 5px">
                  Exploring a new task of generating dexterous grasps with natural language guidance. We propose a dataset to support this task, and design a new model that maintains human intention, high diversity and good quality.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="" target="_blank">
                  <span class="papertitle">Dexterous Grasp Transformer</span>
                </a>
                <br>
                Guo-Hao Xu<sup>&</sup>, Yi-Lin Wei<sup>&</sup>, Dian Zheng, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2404.18135" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">code</a>
                <p style="margin-bottom: 0px; margin-top: 5px">
                  A new transformer-based framework for dexterous grasp generation, capable of predicting a diverse set of feasible grasp poses only in one pass.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="" target="_blank">
                  <span class="papertitle">Single-View Scene Point Cloud Human Grasp Generation</span>
                </a>
                <br>
                Yan-Kang Wang, Chengyi Xing, Yi-Lin Wei, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2404.15815" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/S2HGrasp" target="_blank">code</a>
                <p style="margin-bottom: 0px; margin-top: 5px">
                  Exploring a new task of generating human grasps based on single-view scene point clouds. A new baseline and a new dataset are also proposed for this novel task.
                </p>
              </td>
            </tr>

            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Representation Learning</h3>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle" bgcolor="#ffffd0">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">
                  <span class="papertitle">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Dian Zheng, Zuhao Liu, Wei-Shi Zheng*
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/ReSTE" target="_blank">code</a>
                <p style="margin-bottom: 5px; margin-top: 5px">
                  A new perspective to view the binary neural network training: equilibrium between estimating error and gradient stability. And a simple and effective gradient estimator is proposed to balance it.
                </p>
              </td>
            </tr>

            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Other AI Tasks</h3>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.11157" target="_blank">
                  <span class="papertitle">Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://arxiv.org/abs/2403.11157" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">code</a>
                <p style="margin-bottom: 5px; margin-top: 5px">
                  A diffusion-based universal image restoration model, with an assemble-then-separate (like the hourglass) mapping for multi-tasks training.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2308.15989" target="_blank">
                  <span class="papertitle">DiffuVolume: Diffusion Model for Volume based Stereo Matching</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Zuhao Liu, Jingke Meng, Wei-Shi Zheng*
                <br>
                <em>arXiv preprint arXiv:2308.15989</em>
                <br>
                <a href="https://arxiv.org/abs/2308.15989" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffuVolume" target="_blank">code</a>
                <p style="margin-bottom: 5px; margin-top: 5px">
                  A diffusion-based module to gradually remove the redundancy in the cost volume for stereo matching, which is also plug-and-play for volume-based methods.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.html" target="_blank">
                  <span class="papertitle">Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping</span>
                </a>
                <br>
                Zuhao Liu, <strong>Xiao-Ming Wu</strong>, Dian Zheng, Kun-Yu Lin, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.html" target="_blank">paper</a>
                <p style="margin-bottom: 5px; margin-top: 5px">
                  A prompt-based feature mapping to generate more real-domain anomalies to supervise the anomaly detection training.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3511808.3557488" target="_blank">
                  <span class="papertitle">Weakly-Supervised Online Hashing with Refined Pseudo Tags</span>
                </a>
                <br>
                Chen-Lu Ding, Xin Luo*, <strong>Xiao-Ming Wu</strong>, Yu-Wei Zhan, Rui Li, Hui Zhang, and Xin-Shun Xu
                <br>
                <em>Conference on Information and Knowledge Management (CIKM)</em>, 2022
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3511808.3557488" target="_blank">paper</a>
                /
                <a href="https://github.com/oceanoceanna/WOH-RPT" target="_blank">code</a>
                <p style="margin-bottom: 5px; margin-top: 5px">
                  Generating pseudo tags based on the co-occurrence similarity between tags to handle the weakly-supervised online hashing.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle" bgcolor="#ffffd0">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">
                  <span class="papertitle">Online Enhanced Semantic Hashing Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Xin Luo*, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/OASIS" target="_blank">code</a>
                <p style="margin-bottom: 0px">
                  Introducing semantic into online hashing, handling the dimension mismatching and mitigating the inconsistent problem in incremental online hashing.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding-bottom: 15px; width:25%; vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">
                  <span class="papertitle">Discrete Online Cross-Modal Hashing</span>
                </a>
                <br>
                Yu-Wei Zhan, Yong-Xin Wang, Yu Sun, <strong>Xiao-Ming Wu</strong>, Xin Luo*, and Xin-Shun Xu
                <br>
                <em>Pattern Recognition (PR)</em>, 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">paper</a>
                /
                <a href="https://github.com/yw-zhan/DOCH" target="_blank">code</a>
                <p style="margin-bottom: 0px">
                  Inspired by Discrete Latent Factor Hashing (DLFH), we propose an online-version DLFH, with many novel designs to handle the online problem in cross-modal hashing.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                Academic Service
              </h2>
              <p>
                Journal Reviewer: Pattern Analysis and Machine Intelligence (TPAMI), Pattern Recognition (PR).
              </p>
              <p>
                Conference Reviewer: Computer Vision and Pattern Recognition (CVPR) 2024, ACM Multimedia (MM) 2024.
              </p>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                Awards
              </h2>
              <p>
                <div style="padding-bottom: 5px"> National Scholarship of China for Guaduate Student (研究生国家奖学金), 2023 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Sun Yat-Sen University (中山大学学业一等奖学金), 2022, 2023 </div>
                <div style="padding-bottom: 5px"> Honorable Bachelor Degree of Shandong University (山东大学荣誉学士学位), 2022 </div>
                <div style="padding-bottom: 5px"> National Scholarship of China for Underguaduate Student  (本科生国家奖学金), 2019, 2021 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Shandong University (山东大学学业一等奖学金), 2019, 2020, 2021 </div>
              </p>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
