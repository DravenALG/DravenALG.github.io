<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiao-Ming Wu</title>

    <meta name="author" content="Xiao-Ming Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/Naruto.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">

              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/myself.jpg" target="_blank"><img style="width:85%;max-width:100%;object-fit: cover;border-radius: 75px" alt="profile photo" src="images/myself.jpg" class="hoverZoomLink"></a>
              </td>

              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: left;">
                  Xiao-Ming Wu (伍晓鸣)
                </p>
                <p>
                  M.S. student
                </p>
                <p>
                  School of Computer Science and Engineering
                </p>
                <p>
                  Sun Yat-sen University
                </p>
                <p>
                  wuxm65@mail2.sysu.edu.cn
                </p>
                <p style="text-align:left">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Li7oZpsAAAAJ" target="_blank">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/DravenALG" target="_blank">Github</a>
                </p>
              </td>

            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>
                  Biography
                </h2>
                <p>
                  I'm currently a second-year master student at <a href="https://www.sysu.edu.cn" target="_blank">Sun Yat-sen University</a>, advised by Prof. <a href="https://www.isee-ai.cn/~zhwshi/" target="_blank">Wei-Shi Zheng</a>, where I develop the scientific ability and a good taste of it.
                  Now I am also a visiting student in <a href="https://www.mq.edu.au" target="_blank">Macquarie university</a>, enjoying an interesting research journey with Prof. <a href="https://researchers.mq.edu.au/en/persons/longbing-cao" target="_blank">Longbing Cao</a>.
                  Previously, I obtain my B.E. degree in <a href="https://www.sdu.edu.cn/index.htm" target="_blank">Shandong University</a>. At that time, I had my first attempt on scientific research and cultivate the interest in it, advised by Prof. <a href="http://mima.sdu.edu.cn/Members/xinshunxu/index.htm" target="_blank">Xin-Shun Xu</a> and <a href="https://faculty.sdu.edu.cn/luoxin/zh_CN/index.htm" target="_blank">Xin Luo</a>.
                  <strong>I am now looking for a potential Ph.D. position in 2025 Fall.</strong>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:100%; vertical-align:middle">
                <h2>Research Interests</h2>
                <p>
                  <em class="ref">
                    Research is for curiosity and fun.
                  </em>
                </p>
                <p>
                  My research interest is to build a strong, efficient (reach human-level), and safe (surpass human level) AI agent. And below is the roadmap:
                </p>
                <ul style="padding-left: 20px;">
                  <li><strong>Representation Learning:</strong> design new supervision framework, architecture, or optimizer to learn powerful representation ability. </li>
                  <li style="padding-bottom: 4px"><strong>Embodied AI:</strong> let the AI agent have the embodied ability to interact with and feel the real world.</li>
                </ul>
                <p>
                  I hope to build a model that can learn powerful representation, and further enhance the representation based on the embodied capabilities, hoping to finally build a strong, efficient, and safe AI agent.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 0px; margin-bottom: -10px;"><tbody>
            <tr>
              <td style="width:80%; vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  Below are my publications. My first author works are <span class="highlight">highlighted</span>.
                  (This page includes papers in arXiv, & means equal contribution, * refers to corresponding author.)
                </p>
              </td>
            </tr>
           </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Representation Learning</h3>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/HCFW.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.10776" target="_blank">
                  <span class="papertitle">High-level Codes and Fine-grained Weights for Online Multi-modal Hashing Retrieval</span>
                </a>
                <br>
                Yu-Wei Zhan&, <strong>Xiao-Ming Wu&</strong>, Xin Luo*, Yinwei Wei, Xin-Shun Xu
                <br>
                <em>arXiv preprint arXiv:2406.10776</em>
                <br>
                <a href="https://arxiv.org/abs/2406.10776" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Representation)</strong> We propose a new framework for incremental multi-modal hashing, which consists of the high-level codes technique for consistent hash codes learning and fine-grained weights strategy for multi-modal weights learning.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/ReSTE.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">
                  <span class="papertitle">Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Dian Zheng, Zuhao Liu, Wei-Shi Zheng*
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/ReSTE" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Binary Neural Networks)</strong> We propose a new perspective to view the binary neural network training: equilibrium between estimating error and gradient stability, and design a simple and effective gradient estimator (ReSTE) to balance it well.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/RPT-WOH.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3511808.3557488" target="_blank">
                  <span class="papertitle">Weakly-Supervised Online Hashing with Refined Pseudo Tags</span>
                </a>
                <br>
                Chen-Lu Ding, Xin Luo*, <strong>Xiao-Ming Wu</strong>, Yu-Wei Zhan, Rui Li, Hui Zhang, and Xin-Shun Xu
                <br>
                <em>Conference on Information and Knowledge Management (CIKM)</em>, 2022
                <br>
                <a href="https://dl.acm.org/doi/abs/10.1145/3511808.3557488" target="_blank">paper</a>
                /
                <a href="https://github.com/oceanoceanna/WOH-RPT" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Representation)</strong> We propose a new framework for weakly-supervised online hashing, using the co-occurrence similarity between tags to handle the imperfect user-provided tags well.
                </p>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/OASIS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">
                  <span class="papertitle">Online Enhanced Semantic Hashing Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu</strong>, Xin Luo*, Yu-Wei Zhan, Chen-Lu Ding, Zhen-Duo Chen, Xin-Shun Xu
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20346" target="_blank">paper</a>
                /
                <a href="https://github.com/DravenALG/OASIS" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Representation)</strong> We explore a new task, incremental multi-modal hashing, and introduce semantics to solve this task, handling the dimension mismatching problem and mitigating the inconsistent problem that occurs when new classes come.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DOCH.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">
                  <span class="papertitle">Discrete Online Cross-Modal Hashing</span>
                </a>
                <br>
                Yu-Wei Zhan, Yong-Xin Wang, Yu Sun, <strong>Xiao-Ming Wu</strong>, Xin Luo*, and Xin-Shun Xu
                <br>
                <em>Pattern Recognition (PR)</em>, 2022
                <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004428" target="_blank">paper</a>
                /
                <a href="https://github.com/yw-zhan/DOCH" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Hashing Representation)</strong> We propose an online-version DLFH, with similarity learning and label embedding to handle the online problem in cross-modal hashing, which can generate hash codes in a discrete way to reduce the quantization error.
                </p>
              </td>
            </tr>

            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Embodied AI</h3>
              </td>
            </tr>

            <tr bgcolor="#ffffd0">
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/EconomicGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2407.08366" target="_blank">
                  <span class="papertitle">An Economic Framework for 6-DoF Grasp Detection</span>
                </a>
                <br>
                <strong>Xiao-Ming Wu&</strong>, Jia-Feng Cai&, Jian-Jian Jiang, Dian Zheng, Yi-Lin Wei, Wei-Shi Zheng*
                <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2407.08366" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/EconomicGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(6-DoF Grasp)</strong> We propose a new economic grasping framework for 6-DoF grasp detection to economize the training resource cost and meanwhile maintain effective grasp performance, which consists of a novel label selection strategy and a focal module to enable it.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DexGYS.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">
                  <span class="papertitle">Grasp as You Say: Language-guided Dexterous Grasp Generation</span>
                </a>
                <br>
                Yi-Lin Wei, Jian-Jian Jiang, Chengyi Xing, Xiantuo Tan, <strong>Xiao-Ming Wu</strong>, Hao Li, Mark Cutkosky, Wei-Shi Zheng*
                <br>
                <em>arXiv preprint arXiv:2405.19291</em>
                <br>
                <a href="https://arxiv.org/abs/2405.19291" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dexterous Grasp)</strong> We explore a new task of generating dexterous grasps with natural language guidance.  We propose a dataset to support this task and design a new model that maintains human intention, high diversity, and good quality.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DGTR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Dexterous Grasp Transformer</span>
                </a>
                <br>
                Guo-Hao Xu<sup>&</sup>, Yi-Lin Wei<sup>&</sup>, Dian Zheng, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_Dexterous_Grasp_Transformer_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DGTR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Dexterous Grasp)</strong> We propose a new transformer-based framework for dexterous grasp generation, capable of predicting a diverse set of feasible grasp poses only in one pass.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/S2HGrasp.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Single-View Scene Point Cloud Human Grasp Generation</span>
                </a>
                <br>
                Yan-Kang Wang, Chengyi Xing, Yi-Lin Wei, <strong>Xiao-Ming Wu</strong>, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Single-View_Scene_Point_Cloud_Human_Grasp_Generation_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/S2HGrasp" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Human Grasp)</strong> We explore a new task of generating human grasps based on single-view scene point clouds. And we design a new diffusion-based baseline and a new dataset for this novel task.
                </p>
              </td>
            </tr>

            <tr>
              <td style="width:25%; vertical-align:middle">
                <h3>Other AI Applications</h3>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/PixelFade.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="" target="_blank">
                  <span class="papertitle">PixelFade: Privacy-preserving Person Re-identification with Noise-guided Progressive Replacement</span>
                </a>
                <br>
                Delong Zhang, Yi-Xing Peng, <strong>Xiao-Ming Wu</strong>, Ancong Wu*, Wei-Shi Zheng.
                <br>
                <em>ACM Multimedia (MM)</em>, 2024.
                <br>
                <a href="https://github.com/iSEE-Laboratory/PixelFade" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Privacy Protection)</strong> We propose a two-step iterative method (PixelFade) for person privacy-preserving, with the partial replacement step to turn image into noise to resist recovery attack, and the constrain operation step to maintain Re-id semantics.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DiffUIR.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">
                  <span class="papertitle">Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Shuzhou Yang, Jian Zhang, Jian-Fang Hu, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024.
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Selective_Hourglass_Mapping_for_Universal_Image_Restoration_Based_on_Diffusion_CVPR_2024_paper.html" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffUIR" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Image Restoration)</strong> We propose a diffusion-based universal image restoration model, with an assemble-then-separate (like the hourglass) mapping for multi-task training, to learn the shared information between different tasks.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/DiffuVolume.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://arxiv.org/abs/2308.15989" target="_blank">
                  <span class="papertitle">DiffuVolume: Diffusion Model for Volume based Stereo Matching</span>
                </a>
                <br>
                Dian Zheng, <strong>Xiao-Ming Wu</strong>, Zuhao Liu, Jingke Meng, Wei-Shi Zheng*
                <br>
                <em>arXiv preprint arXiv:2308.15989</em>
                <br>
                <a href="https://arxiv.org/abs/2308.15989" target="_blank">paper</a>
                /
                <a href="https://github.com/iSEE-Laboratory/DiffuVolume" target="_blank">code</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Stereo Matching)</strong> We propose a diffusion-based module to gradually remove the redundancy in the cost volume for stereo matching, which is also plug-and-play for volume-based methods.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding: 20px; width:25%;vertical-align:middle">
                <img src="images/PFMF.png" alt="PontTuset" width="200" style="border-style: none">
              </td>
              <td style="width:75%; vertical-align:middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.html" target="_blank">
                  <span class="papertitle">Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping</span>
                </a>
                <br>
                Zuhao Liu, <strong>Xiao-Ming Wu</strong>, Dian Zheng, Kun-Yu Lin, Wei-Shi Zheng*
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.html" target="_blank">paper</a>
                <p style="margin-bottom: 15px; margin-top: 5px">
                  <strong>(Anomaly Detection)</strong> We propose a prompt-based feature mapping to generate more real-domain anomalies to supervise the anomaly detection training better.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                Academic Service
              </h2>
              <p>
                Journal Reviewer: Pattern Analysis and Machine Intelligence (TPAMI), Pattern Recognition (PR).
              </p>
              <p>
                Conference Reviewer: Computer Vision and Pattern Recognition (CVPR) 2024, ACM Multimedia (MM) 2024.
              </p>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-left:20px; padding-bottom: 20px;"><tbody>
          <tr>
            <td style="width:100%; vertical-align:middle">
              <h2>
                Awards
              </h2>
              <p>
                <div style="padding-bottom: 5px"> National Scholarship of China for Graduate Student (研究生国家奖学金), 2023 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Sun Yat-Sen University (中山大学学业一等奖学金), 2022, 2023 </div>
                <div style="padding-bottom: 5px"> Honorable Bachelor Degree of Shandong University (山东大学荣誉学士学位), 2022 </div>
                <div style="padding-bottom: 5px"> National Scholarship of China for Undergraduate Student  (本科生国家奖学金), 2019, 2021 </div>
                <div style="padding-bottom: 5px"> First Prize, Academic Scholarship of Shandong University (山东大学学业一等奖学金), 2019, 2020, 2021 </div>
              </p>
            </td>
          </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
